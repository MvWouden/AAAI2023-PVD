## PVD-AL: Progressive Volume Distillation with Active Learning for Efficient Conversion Between Different NeRF Architectures


## [Project Page](http://sk-fun.fun/PVD-AL/) | [Paper](https://arxiv.org/abs/2304.04012) | [Datasets](https://drive.google.com/drive/folders/1U06KAEsW53PolLI3U8hWUhzzIH74QGaP?usp=sharing) | [Ckpts](https://drive.google.com/drive/folders/1GGJf-FTmpCJjmEn-AF_S9-HrLRkFe5Ud?usp=sharing) |



<img width="1005" alt="image" src="https://user-images.githubusercontent.com/34268707/231034579-a1beb97a-2aa4-469f-9bcd-36d3a83bfd7b.png">


This code is based on the [PVD](https://github.com/megvii-research/AAAI2023-PVD), we additionally introduce an active learning strategy to take the PVD performance one step further, which provides a more comprehensive and deeper understanding of distillation between different architectures.

### The complete code will be released in May
